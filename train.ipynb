{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#installing nesseccary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.8/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.venv/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.8/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.8/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.8/site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define a function to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    files = [\"processedNegative.csv\", \"processedPositive.csv\", \"processedNeutral.csv\"]\n",
    "    try:\n",
    "        df = pd.DataFrame(columns=[\"text\", \"sentiment\"])\n",
    "        for file in files:\n",
    "            with open(\"data/\" + file, \"r\") as f:\n",
    "                content = f.read()\n",
    "                tweets = content.split(\",\")\n",
    "                tmp_df = pd.DataFrame(tweets, columns=[\"text\"])\n",
    "                sentiment = file[9:-4]\n",
    "                tmp_df[\"sentiment\"] = sentiment\n",
    "                df = pd.concat([df, tmp_df])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call load_data() and print a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0              How unhappy  some dogs like it though  Negative\n",
      "1  talking to my over driver about where I'm goin...  Negative\n",
      "2  Does anybody know if the Rand's likely to fall...  Negative\n",
      "3         I miss going to gigs in Liverpool unhappy   Negative\n",
      "4      There isnt a new Riverdale tonight ? unhappy   Negative\n"
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define textpreprocessor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocess(self, text, config):\n",
    "        \"\"\"\n",
    "        config = {\n",
    "            'case': 'original' or 'lower',\n",
    "            'special_chars': 'keep' or 'remove',\n",
    "            'stop_words': 'keep' or 'remove',\n",
    "            'normalization': 'none' or 'stem' or 'lemma'\n",
    "        }\n",
    "        \"\"\"\n",
    "        # Case handling\n",
    "        if config['case'] == 'lower':\n",
    "            text = text.lower()\n",
    "\n",
    "        # Special characters\n",
    "        if config['special_chars'] == 'remove':\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Tokenization\n",
    "        words = text.split()\n",
    "\n",
    "        # Stop words\n",
    "        if config['stop_words'] == 'remove':\n",
    "            words = [w for w in words if w not in self.stop_words]\n",
    "\n",
    "        # Normalization\n",
    "        if config['normalization'] == 'stem':\n",
    "            words = [self.stemmer.stem(w) for w in words]\n",
    "        elif config['normalization'] == 'lemma':\n",
    "            words = [self.lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "        return ' '.join(words)\n",
    "\n",
    "    # Generate all 18 combinations\n",
    "    def get_preprocessing_configs(self):\n",
    "        configs = []\n",
    "        for case in ['original', 'lower']:\n",
    "            for special_chars in ['keep', 'remove']:\n",
    "                for stop_words in ['keep', 'remove']:\n",
    "                    for norm in ['none', 'stem', 'lemma']:\n",
    "                        configs.append({\n",
    "                            'case': case,\n",
    "                            'special_chars': special_chars,\n",
    "                            'stop_words': stop_words,\n",
    "                            'normalization': norm\n",
    "                        })\n",
    "        return configs[:2]  # Return first 18 combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "configs = preprocessor.get_preprocessing_configs()\n",
    "\n",
    "# Process tweets with each configuration\n",
    "processed_datasets = []\n",
    "for config in configs:\n",
    "    processed_df = df.copy()\n",
    "    processed_df['text'] = processed_df['text'].apply(\n",
    "        lambda x: preprocessor.preprocess(x, config)\n",
    "    )\n",
    "    processed_datasets.append((config, processed_df))\n",
    "\n",
    "# Save each processed dataset\n",
    "for i, (config, dataset) in enumerate(processed_datasets):\n",
    "    dataset.to_csv(f'processed_dataset_{i+1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_preprocessed_datasets(df, preprocessor):\n",
    "    # Get all preprocessing configurations\n",
    "    configs = preprocessor.get_preprocessing_configs()\n",
    "    datasets = []\n",
    "    \n",
    "    for i, config in enumerate(configs, 1):\n",
    "        print(f\"Creating dataset {i}/18 with config: {config}\")\n",
    "        processed_df = df.copy()\n",
    "        processed_df['text'] = processed_df['text'].apply(\n",
    "            lambda x: preprocessor.preprocess(x, config)\n",
    "        )\n",
    "        datasets.append((f\"dataset_{i}\", processed_df, config))\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#vectorize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def VectorizeWords(DataFrame: pd.DataFrame) -> Tuple[TfidfVectorizer, np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Convert text data to TF-IDF vectors\n",
    "    \n",
    "    Args:\n",
    "        DataFrame: pandas DataFrame containing 'text' and 'sentiment' columns\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - fitted TfidfVectorizer\n",
    "        - transformed text data as numpy array\n",
    "        - sentiment labels as pandas Series\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(DataFrame['text']).toarray()\n",
    "    y = DataFrame['sentiment']\n",
    "    \n",
    "    return vectorizer, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(datasets):\n",
    "    prepared_data = []\n",
    "    \n",
    "    for name, df, config in datasets:\n",
    "        # Vectorize\n",
    "        vectorizer, X, y = VectorizeWords(df)\n",
    "        \n",
    "        # Split into train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        prepared_data.append({\n",
    "            'name': name,\n",
    "            'config': config,\n",
    "            'vectorizer': vectorizer,\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test\n",
    "        })\n",
    "    \n",
    "    return prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_and_evaluate(prepared_data):\n",
    "    results = []\n",
    "    \n",
    "    for data in prepared_data:\n",
    "        print(f\"\\nProcessing {data['name']}...\")\n",
    "        \n",
    "        # Perform GridSearch\n",
    "        best_models = perform_gridsearch(data['X_train'], data['y_train'])\n",
    "        \n",
    "        # Evaluate each model\n",
    "        for model_name, model_info in best_models.items():\n",
    "            best_model = model_info['best_model']\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = best_model.predict(data['X_test'])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(data['y_test'], y_pred)\n",
    "            \n",
    "            results.append({\n",
    "                'dataset': data['name'],\n",
    "                'preprocessing_config': data['config'],\n",
    "                'model': model_name,\n",
    "                'best_params': model_info['best_params'],\n",
    "                'accuracy': accuracy,\n",
    "                'model_object': best_model,\n",
    "                'vectorizer': data['vectorizer']\n",
    "            })\n",
    "            \n",
    "            print(f\"{model_name} Accuracy: {accuracy:.3f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(data['y_test'], y_pred))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def perform_gridsearch(X_train, y_train):\n",
    "    # Define models and their parameter grids\n",
    "    models = {\n",
    "        'svm': {\n",
    "            'model': SVC(),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'gamma': ['scale', 'auto', 0.1, 1],\n",
    "            }\n",
    "        }#,\n",
    "        # 'random_forest': {\n",
    "        #     'model': RandomForestClassifier(),\n",
    "        #     'params': {\n",
    "        #         'n_estimators': [100, 200, 300],\n",
    "        #         'max_depth': [10, 20, None],\n",
    "        #         'min_samples_split': [2, 5, 10],\n",
    "        #         'min_samples_leaf': [1, 2, 4]\n",
    "        #     }\n",
    "        # },\n",
    "        # 'logistic_regression': {\n",
    "        #     'model': LogisticRegression(),\n",
    "        #     'params': {\n",
    "        #         'C': [1.0],#[0.01, 0.1, 1, 10],\n",
    "        #         'penalty': ['l2'],#['l1', 'l2'],\n",
    "        #         'solver': ['liblinear', 'saga'],\n",
    "        #         'max_iter': [1000]\n",
    "        #     }\n",
    "        # }\n",
    "    }\n",
    "    \n",
    "    best_models = {}\n",
    "    \n",
    "    # Perform GridSearch for each model\n",
    "    for model_name, model_info in models.items():\n",
    "        print(f\"\\nPerforming GridSearch for {model_name}...\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model_info['model'],\n",
    "            param_grid=model_info['params'],\n",
    "            cv=5,                # 5-fold cross-validation\n",
    "            scoring='f1_macro',   # Using AUC score\n",
    "            n_jobs=-1,          # Use all CPU cores\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit the grid search\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Store results\n",
    "        best_models[model_name] = {\n",
    "            'best_model': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_\n",
    "        }\n",
    "        \n",
    "        print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        print(f\"Best AUC score: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_and_evaluate(prepared_data):\n",
    "    results = []\n",
    "    \n",
    "    for data in prepared_data:\n",
    "        print(f\"\\nProcessing {data['name']}...\")\n",
    "        \n",
    "        # Perform GridSearch\n",
    "        best_models = perform_gridsearch(data['X_train'], data['y_train'])\n",
    "        \n",
    "        # Evaluate each model\n",
    "        for model_name, model_info in best_models.items():\n",
    "            best_model = model_info['best_model']\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = best_model.predict(data['X_test'])\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(data['y_test'], y_pred)\n",
    "            \n",
    "            results.append({\n",
    "                'dataset': data['name'],\n",
    "                'preprocessing_config': data['config'],\n",
    "                'model': model_name,\n",
    "                'best_params': model_info['best_params'],\n",
    "                'accuracy': accuracy,\n",
    "                'model_object': best_model,\n",
    "                'vectorizer': data['vectorizer']\n",
    "            })\n",
    "            \n",
    "            print(f\"{model_name} Accuracy: {accuracy:.3f}\")\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(data['y_test'], y_pred))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Save a model to a file using pickle\n",
    "    \n",
    "    Args:\n",
    "        model: The model object to save\n",
    "        filename: The path where to save the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Model successfully saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model to {filename}: {str(e)}\")\n",
    "\n",
    "# And here's how to load the model later when you need it:\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Load a model from a file\n",
    "    \n",
    "    Args:\n",
    "        filename: The path to the saved model file\n",
    "    \n",
    "    Returns:\n",
    "        The loaded model object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model successfully loaded from {filename}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {filename}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating preprocessed datasets...\n",
      "Creating dataset 1/18 with config: {'case': 'original', 'special_chars': 'keep', 'stop_words': 'keep', 'normalization': 'none'}\n",
      "Creating dataset 2/18 with config: {'case': 'original', 'special_chars': 'keep', 'stop_words': 'keep', 'normalization': 'stem'}\n",
      "\n",
      "Preparing datasets for training...\n",
      "\n",
      "Training and evaluating models...\n",
      "\n",
      "Processing dataset_1...\n",
      "\n",
      "Performing GridSearch for svm...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for svm: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best AUC score: 0.878\n",
      "svm Accuracy: 0.901\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.96      0.86      0.91       224\n",
      "     Neutral       0.85      0.97      0.91       314\n",
      "    Positive       0.93      0.84      0.88       237\n",
      "\n",
      "    accuracy                           0.90       775\n",
      "   macro avg       0.91      0.89      0.90       775\n",
      "weighted avg       0.91      0.90      0.90       775\n",
      "\n",
      "\n",
      "Processing dataset_2...\n",
      "\n",
      "Performing GridSearch for svm...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     save_model(best_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_vectorizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train and evaluate models\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining and evaluating models...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Find best performing model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m best_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(prepared_data)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Perform GridSearch\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m best_models \u001b[38;5;241m=\u001b[39m \u001b[43mperform_gridsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Evaluate each model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_info \u001b[38;5;129;01min\u001b[39;00m best_models\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[11], line 53\u001b[0m, in \u001b[0;36mperform_gridsearch\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m     43\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     44\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mmodel_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     45\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mmodel_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Fit the grid search\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m     56\u001b[0m best_models[model_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m'\u001b[39m: grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_,\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m: grid_search\u001b[38;5;241m.\u001b[39mbest_params_,\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_score\u001b[39m\u001b[38;5;124m'\u001b[39m: grid_search\u001b[38;5;241m.\u001b[39mbest_score_\n\u001b[1;32m     60\u001b[0m }\n",
      "File \u001b[0;32m~/Desktop/1337/tweets/.venv/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/1337/tweets/.venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/1337/tweets/.venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/1337/tweets/.venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/1337/tweets/.venv/lib/python3.8/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/1337/tweets/.venv/lib/python3.8/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/1337/tweets/.venv/lib/python3.8/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/1337/tweets/.venv/lib/python3.8/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load your data using the previously defined load_data function\n",
    "    df = load_data()\n",
    "    if df is None:\n",
    "        print(\"Error loading data. Please check your data files.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = TextPreprocessor()\n",
    "    \n",
    "    # Create different preprocessed versions\n",
    "    print(\"Creating preprocessed datasets...\")\n",
    "    datasets = create_preprocessed_datasets(df, preprocessor)\n",
    "    \n",
    "    # Prepare datasets for training\n",
    "    print(\"\\nPreparing datasets for training...\")\n",
    "    prepared_data = prepare_datasets(datasets)\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    results = train_and_evaluate(prepared_data)\n",
    "    \n",
    "    # Find best performing model\n",
    "    best_result = max(results, key=lambda x: x['accuracy'])\n",
    "    print(\"\\nBest performing model:\")\n",
    "    print(f\"Dataset: {best_result['dataset']}\")\n",
    "    print(f\"Preprocessing config: {best_result['preprocessing_config']}\")\n",
    "    print(f\"Model: {best_result['model']}\")\n",
    "    print(f\"Accuracy: {best_result['accuracy']:.3f}\")\n",
    "    \n",
    "    # Save best model and vectorizer\n",
    "    save_model(best_result['model_object'], 'best_model.pkl')\n",
    "    save_model(best_result['vectorizer'], 'best_vectorizer.pkl')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
